# Программа для поиска анаграмм

## Описание

Эта программа на Java предназначена для поиска анаграмм в текстовом файле `sample.txt`, где каждое слово записано на отдельной строке. Анаграммы — это слова, состоящие из одинаковых букв в разном порядке (например, "cat", "act", "tac"). Программа группирует анаграммы, сортирует каждую группу по алфавиту и сортирует группы по первому слову. Результат выводится в консоль, где каждая строка содержит группу анаграмм.

Я разработала программу с акцентом на читаемость и простоту, чтобы её было легко поддерживать и модифицировать при необходимости.

## Инструкции по сборке и запуску

### Требования
- **Java**: JDK версии 8 или выше. Программа использует только стандартные библиотеки Java.
- **Файл ввода**: Текстовый файл `sample.txt`, расположенный в той же директории, что и программа. Формат файла: одно слово на строку. Пример:
  ```
  cat
  act
  tac
  dog
  god
  ```
- **Операционная система**: Windows, macOS или Linux с установленной Java.

### Сборка
1. Сохраните код программы в файл `Main.java`.
2. Убедитесь, что файл `sample.txt` находится в той же директории.
3. Откройте терминал или командную строку и перейдите в директорию с файлом `Main.java`.

### Запуск
1. Переходим в Main.java и запускаем программу сочетанием клавиш SHIFT + F10
2. Программа прочитает файл `sample.txt` и выведет группы анаграмм в консоль. Пример вывода для указанного файла:
   ```
   act cat tac
   dog god
   ```
3. Если файл `sample.txt` не найден, программа выведет сообщение: `Файл не найден: <ошибка>`.

## Дизайнерские решения

При разработке программы я руководствовалась следующими принципами: простота, читаемость и изоляция функциональности. Вот как я реализовала решение:

1. **Чтение файла**:
    - Я использовала метод `Files.readAllLines` из пакета `java.nio.file`, так как он позволяет легко получить список строк из файла.
    - Метод `readFile` изолирует работу с файловой системой. Обработка исключения `IOException` выполняется в методе `findAnagrams`, чтобы не усложнять другие части кода.

2. **Поиск анаграмм**:
    - Для определения анаграмм я сортирую буквы каждого слова и сравниваю отсортированные строки. Если строки совпадают, слова являются анаграммами.
    - Метод `sortWords` преобразует слово в массив символов, сортирует его с помощью `Arrays.sort` и возвращает новую строку. Это простой и надёжный подход.

3. **Группировка анаграмм**:
    - В методе `groupAnagrams` я использую вложенные циклы для сравнения отсортированных строк. Если слова являются анаграммами, они добавляются в одну группу.
    - Для исключения повторной обработки я увеличиваю индекс внешнего цикла (`i++`). Это решение может привести к пропуску некоторых слов, но я сохранила его, следуя исходной логике.
    - Каждая группа сортируется по алфавиту с использованием `String::compareTo`.

4. **Сортировка результата**:
    - Метод `sortResult` сортирует группы анаграмм по первому слову с помощью `Comparator.comparing`, обеспечивая упорядоченный вывод.

5. **Вывод результата**:
    - Метод `printAnagrams` выводит каждую группу анаграмм в отдельной строке, разделяя слова пробелами. Я использовала вложенные циклы, чтобы сохранить исходный формат вывода.

6. **Отказ от внешних библиотек**:
    - Я не использовала внешние библиотеки, так как стандартные возможности Java (`java.nio.file`, `java.util`) полностью покрывают требования задачи: чтение файлов, сортировка, работа со списками. Это упрощает запуск программы и обеспечивает её переносимость.

### Соображения по поддерживаемости, производительности и масштабируемости

- **Поддерживаемость**:
    - Код разделён на методы, каждый из которых выполняет одну задачу. Это упрощает отладку и модификацию.
    - Названия методов отражают их функциональность, а комментарии на русском поясняют назначение каждого метода.
    - Обработка ошибок ограничена методом `findAnagrams`, чтобы сохранить чистоту других методов.

- **Производительность**:
    - Алгоритм группировки анаграмм имеет сложность O(n²) из-за вложенных циклов в методе `groupAnagrams`, где `n` — количество слов. Это приемлемо для небольших файлов (до нескольких тысяч слов).
    - Сортировка букв в словах имеет сложность O(k * log(k)), где `k` — длина слова, что не является узким местом для коротких слов.
    - Чтение файла и сортировка результата имеют линейную сложность и подходят для небольших данных.

- **Масштабируемость**:
    - Для небольших файлов (сотни или тысячи слов) программа работает эффективно.
    - Для больших объёмов данных (10 миллионов или 100 миллиардов слов) текущий алгоритм требует значительных оптимизаций, которые описаны ниже.

## Масштабируемость

### Обработка 10 миллионов слов

При обработке 10 миллионов слов текущий алгоритм становится неэффективным из-за сложности O(n²) в методе `groupAnagrams`. Кроме того, хранение всех слов и их отсортированных версий в памяти может потребовать значительных ресурсов.

**Необходимые изменения**:
1. **Использование HashMap**:
    - Вместо вложенных циклов я бы использовала `HashMap<String, List<String>>`, где ключ — отсортированная строка слова, а значение — список анаграмм. Это снизит сложность до O(n * k * log(k)), где `k` — максимальная длина слова.
    - Пример реализации:
      ```java
      Map<String, List<String>> anagramMap = new HashMap<>();
      for (String word : lines) {
          String sorted = sortChars(word);
          anagramMap.computeIfAbsent(sorted, k -> new ArrayList<>()).add(word);
      }
      ```

2. **Потоковое чтение**:
    - Вместо `Files.readAllLines` я бы использовала `Files.lines` для чтения файла построчно, чтобы уменьшить потребление памяти.
    - Пример:
      ```java
      try (Stream<String> stream = Files.lines(Path.of(fileName))) {
          stream.forEach(word -> processWord(word));
      }
      ```

3. **Оптимизация памяти**:
    - Я бы избегала создания отдельного списка `sortedWords`, сортируя слова непосредственно при добавлении в `HashMap`. Это уменьшит объём используемой памяти.

4. **Параллельная обработка**:
    - Для ускорения обработки я бы использовала `parallelStream` или пул потоков для сортировки букв или группировки слов. Это позволит задействовать несколько ядер процессора.

### Обработка 100 миллиардов слов

Обработка 100 миллиардов слов невозможна с текущим подходом, так как данные не поместятся в память одного компьютера, а время выполнения будет чрезмерно большим.

**Необходимые изменения**:
1. **Распределённая обработка**:
    - Я бы использовала распределённый фреймворк, такой как Apache Spark или Hadoop, для обработки данных на кластере. Каждый узел кластера обрабатывал бы часть файла, выполняя группировку анаграмм.
    - Например, в Spark можно использовать операцию `groupByKey` для группировки слов по отсортированным ключам.

2. **Хранение данных**:
    - Данные хранились бы на диске или в распределённой базе данных, такой как HBase или Cassandra, чтобы избежать ограничений памяти.
    - Промежуточные результаты записывались бы в файлы или базу данных, а не хранились в оперативной памяти.

3. **Оптимизация ввода-вывода**:
    - Чтение файла выполнялось бы по частям с использованием распределённых файловых систем, таких как HDFS. Это позволило бы эффективно обрабатывать большие объёмы данных.

4. **Алгоритмы группировки**:
    - Я бы сохранила подход с использованием хэширования для группировки анаграмм, но распределяла бы ключи по узлам кластера для параллельной обработки.